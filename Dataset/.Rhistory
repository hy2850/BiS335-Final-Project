data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size)
boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 5000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size)
boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.01)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
library(parallel)
detectCores()
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 2)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 3)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 1000
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 4)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
tree.num
boost.pred
length(data.train)
length(data.test)
dim(data.train)
dim(data.test)
480/5
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 500
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 4)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 500
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 1)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 500
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 2)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
tree_size <- 500
error_rate <- c()
for(i in 1:K){
fold_ind <- folds[[i]]
data.train <- data[-fold_ind, ]
data.test <- data[fold_ind, ]
boost.model <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = tree_size, shrinkage = 0.001, interaction.dept = 3)
tree.num <- gbm.perf(boost.fit)
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
#   boost.pred <- predict(boost.model, newdata=data.test, n.trees = tree_size)
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
tree.num
### Tree Boosting
rm(list = ls()) # Clear all workspace
setwd("C:/BiS335 Final Project/Dataset")
## Environment settings
# setwd to directory containing below rds files!
clin <- readRDS("clinical.rds")
mut <- readRDS("mutation.rds")
gex <- readRDS("expression.rds")
genes <- read.csv("gex_anova_result.csv")
mutat <- read.csv("mut_chisq_result.csv")
surv_ind <- clin$survival_index
genes <- genes[order(genes[,2]),]
mutat <- mutat[order(mutat[,2]),]
library(MASS)
common_id <- intersect(intersect(clin$sample_id, mut$sample_id), colnames(gex)) # reduce data
gex <- gex[which(rownames(gex) %in%  genes[,1]), match( common_id,colnames(gex))]
gex <- na.omit(gex)
mut <- mut[which(mut[,1] %in% common_id),c(1,2)]
mut <- mut[which(mut[,2] %in% mutat[,1]), ]
# how do we handle na in gex?
# make binary matrix for mutation
binary <- matrix(data = 0, nrow = length(common_id), ncol = length(mutat[,1]))
colnames(binary) <- paste0(mutat[,1], 'mut')
for (i in 1:length(mutat[,1])){
binary[match(mut[,1][mut[,2]==mutat[i,1]], common_id), i] <- 1
}
# data frame
data <- data.frame(sample_id = common_id)
data$surv_ind <- surv_ind[match(common_id, clin$sample_id)]
data <- cbind(data,t(gex))
data <- cbind(data, binary)
# Fixing bad characters in the column names
column_names <- matrix(data = colnames(data), ncol = length(colnames(data)))
colnames(data)<- as.vector(apply(column_names, 2, FUN = make.names))
#########################################################################################################
# Boosting (Using built-in k-fold CV method and gbm.perf)
library(gbm)
set.seed(1)
train <- sample(1:nrow(data), nrow(data)*0.75)
data.train <- data[train,]
data.test <- data[-train,]
# "Bernouli" distribution -> For 0,1 response / Use "multinomial" distribution for 1~4 response classification?
# predict.gbm -> returns probability? to be classified into each class defined in gbm model with "multinomial" dist
# Q. How to control the parameters? n.trees, interaction.dept, and shrinkage?
#   -> 1. n.trees can be found out using "gbm.perf"
boost.fit <- gbm(surv_ind ~.-sample_id, data = data.train, distribution = "multinomial", n.trees = 500, shrinkage = 0.001, interaction.dept = 4, cv.folds = 5)
tree.num <- gbm.perf(boost.fit, method = "cv")
boost.pred <- predict(boost.fit, newdata = data.test, n.trees = tree.num, type = "response")
boost.test <-apply(boost.pred, 1, which.max) # select class with highest probability
survival_index <- data.test$surv_ind
truth.table <- table(boost.test, survival_index)
error_rate <- 1-sum(diag(truth.table))/sum(truth.table)
error_rate
install.packages("randomForest")
100:100:1000
help(seq)
seq
seq(100,1000,100)
### Bagging tree
rm(list = ls()) # Clear all workspace
setwd("C:/BiS335 Final Project/Dataset")
## Environment settings
# setwd to directory containing below rds files!
clin <- readRDS("clinical.rds")
mut <- readRDS("mutation.rds")
gex <- readRDS("expression.rds")
genes <- read.csv("gex_anova_result.csv")
mutat <- read.csv("mut_chisq_result.csv")
surv_ind <- clin$survival_index
genes <- genes[order(genes[,2]),]
mutat <- mutat[order(mutat[,2]),]
library(MASS)
common_id <- intersect(intersect(clin$sample_id, mut$sample_id), colnames(gex)) # reduce data
gex <- gex[which(rownames(gex) %in%  genes[,1]), match( common_id,colnames(gex))]
gex <- na.omit(gex)
mut <- mut[which(mut[,1] %in% common_id),c(1,2)]
mut <- mut[which(mut[,2] %in% mutat[,1]), ]
# how do we handle na in gex?
# make binary matrix for mutation
binary <- matrix(data = 0, nrow = length(common_id), ncol = length(mutat[,1]))
colnames(binary) <- paste0(mutat[,1], 'mut')
for (i in 1:length(mutat[,1])){
binary[match(mut[,1][mut[,2]==mutat[i,1]], common_id), i] <- 1
}
# data frame
data <- data.frame(sample_id = common_id)
data$surv_ind <- surv_ind[match(common_id, clin$sample_id)]
data <- cbind(data,t(gex))
data <- cbind(data, binary)
# Fixing bad characters in the column names
column_names <- matrix(data = colnames(data), ncol = length(colnames(data)))
colnames(data)<- as.vector(apply(column_names, 2, FUN = make.names))
## Fitting a bagging tree
library(randomForest)
bag.fit <- randomForest(surv_ind ~.-sample_id, data = data, mtry = 868, importance = TRUE)
{par(mfrow = c(1,1))
plot(bag.fit)
bag.fit}
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
error_mean_per <- seq(100,1000,100)
tree_size <- 100:
for (size in tree_size){
error_rate <- c()
for(i in 1:K){6
test.ind <- folds[[i]]
train.ind <- setdiff(obs_num, test.ind)
bag.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = 868, importance = TRUE, ntree = size)
bag.test <- predict(bag.model, newdata=data[test.ind,], type = "class")
survival_index <- data[test.ind,]$surv_ind
truth.table <- table(bag.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_mean_per <- append(error_mean_per, mean(error_rate))
}
tree_size; error_mean_per;
## <ntree 吏곸젒 ?젙?빐二쇰뒗 寃쎌슦>
# error_rate <- c()
# for(i in 1:K){
#   test.ind <- folds[[i]]
#   train.ind <- setdiff(obs_num, test.ind)
#
#   #  fold.ind <- folds[[i]]
#   #  data.train <- data[-fold_ind, ]
#   #  data.test <- data[fold_ind, ]
#
#   bag.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = 868, importance = TRUE, ntree = 200)
#
#   bag.test <- predict(bag.model, newdata=data[test.ind,], type = "class")
#   survival_index <- data[test.ind,]$surv_ind
#   truth.table <- table(bag.test, survival_index)
#
#   error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
# }
#
# error_rate; mean(error_rate); sd(error_rate)
## Random Forest
error_rate <- c()
for(i in 1:K){
test.ind <- folds[[i]]
train.ind <- setdiff(obs_num, test.ind)
#  fold.ind <- folds[[i]]
#  data.train <- data[-fold_ind, ]
#  data.test <- data[fold_ind, ]
rand.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = sqrt(868), importance = TRUE)
rand.test <- predict(rand.model, newdata=data[test.ind,], type = "class")
survival_index <- data[test.ind,]$surv_ind
truth.table <- table(rand.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_rate; mean(error_rate); sd(error_rate)
### Bagging tree
rm(list = ls()) # Clear all workspace
setwd("C:/BiS335 Final Project/Dataset")
## Environment settings
# setwd to directory containing below rds files!
clin <- readRDS("clinical.rds")
mut <- readRDS("mutation.rds")
gex <- readRDS("expression.rds")
genes <- read.csv("gex_anova_result.csv")
mutat <- read.csv("mut_chisq_result.csv")
surv_ind <- clin$survival_index
genes <- genes[order(genes[,2]),]
mutat <- mutat[order(mutat[,2]),]
library(MASS)
common_id <- intersect(intersect(clin$sample_id, mut$sample_id), colnames(gex)) # reduce data
gex <- gex[which(rownames(gex) %in%  genes[,1]), match( common_id,colnames(gex))]
gex <- na.omit(gex)
mut <- mut[which(mut[,1] %in% common_id),c(1,2)]
mut <- mut[which(mut[,2] %in% mutat[,1]), ]
# how do we handle na in gex?
# make binary matrix for mutation
binary <- matrix(data = 0, nrow = length(common_id), ncol = length(mutat[,1]))
colnames(binary) <- paste0(mutat[,1], 'mut')
for (i in 1:length(mutat[,1])){
binary[match(mut[,1][mut[,2]==mutat[i,1]], common_id), i] <- 1
}
# data frame
data <- data.frame(sample_id = common_id)
data$surv_ind <- surv_ind[match(common_id, clin$sample_id)]
data <- cbind(data,t(gex))
data <- cbind(data, binary)
# Fixing bad characters in the column names
column_names <- matrix(data = colnames(data), ncol = length(colnames(data)))
colnames(data)<- as.vector(apply(column_names, 2, FUN = make.names))
## Fitting a bagging tree
library(randomForest)
bag.fit <- randomForest(surv_ind ~.-sample_id, data = data, mtry = 868, importance = TRUE)
{par(mfrow = c(1,1))
plot(bag.fit)
bag.fit}
## Cross-validation (K-folds)
# load the library
library(caret)
K <- 5 # Number of folds of the cross-validation
obs_num <- 1:nrow(data)
folds <- createFolds(obs_num, K) # Requires 'caret' package; Creates K-folds for testing (Using rest of the data for training)
error_mean_per <- c()
tree_size <- seq(100,1000,25)
for (size in tree_size){
error_rate <- c()
for(i in 1:K){6
test.ind <- folds[[i]]
train.ind <- setdiff(obs_num, test.ind)
bag.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = 868, importance = TRUE, ntree = size)
bag.test <- predict(bag.model, newdata=data[test.ind,], type = "class")
survival_index <- data[test.ind,]$surv_ind
truth.table <- table(bag.test, survival_index)
error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
}
error_mean_per <- append(error_mean_per, mean(error_rate))
}
tree_size; error_mean_per;
## <ntree 吏곸젒 ?젙?빐二쇰뒗 寃쎌슦>
# error_rate <- c()
# for(i in 1:K){
#   test.ind <- folds[[i]]
#   train.ind <- setdiff(obs_num, test.ind)
#
#   #  fold.ind <- folds[[i]]
#   #  data.train <- data[-fold_ind, ]
#   #  data.test <- data[fold_ind, ]
#
#   bag.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = 868, importance = TRUE, ntree = 200)
#
#   bag.test <- predict(bag.model, newdata=data[test.ind,], type = "class")
#   survival_index <- data[test.ind,]$surv_ind
#   truth.table <- table(bag.test, survival_index)
#
#   error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
# }
#
# error_rate; mean(error_rate); sd(error_rate)
# ## Random Forest
# error_rate <- c()
# for(i in 1:K){
#   test.ind <- folds[[i]]
#   train.ind <- setdiff(obs_num, test.ind)
#
#   #  fold.ind <- folds[[i]]
#   #  data.train <- data[-fold_ind, ]
#   #  data.test <- data[fold_ind, ]
#
#   rand.model <- randomForest(surv_ind ~.-sample_id, data = data, subset = train.ind, mtry = sqrt(868), importance = TRUE)
#
#   rand.test <- predict(rand.model, newdata=data[test.ind,], type = "class")
#   survival_index <- data[test.ind,]$surv_ind
#   truth.table <- table(rand.test, survival_index)
#
#   error_rate <- append(error_rate, 1-sum(diag(truth.table))/sum(truth.table))
# }
#
# error_rate; mean(error_rate); sd(error_rate)
tree_size; error_mean_per;
tree_size;
error_mean_per
help(tuneRF)
which.min(error_mean_per)
tree_size[30]
error_mean_per[30]
